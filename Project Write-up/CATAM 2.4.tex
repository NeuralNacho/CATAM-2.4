\documentclass[12pt, a4paper]{article}

\usepackage[medium]{titlesec}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[legalpaper, portrait, margin=1.2in]{geometry}
\usepackage{wrapfig}
\usepackage{varwidth}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{bm}

\allowdisplaybreaks  % will allow page break in align environment

\begin{document}
	
\setlength{\parindent}{0pt}
\captionsetup{justification=centering}
\lstset{
	showstringspaces=false
}


\begin{titlepage}
	\LARGE
	\textbf{2.4}
	\begin{center}
		\vspace*{7cm}
		
		\LARGE
		\textbf{Additional Project \\ Linear Regression â€” Autocorrelation}
		
		\vspace{0.5cm}
	\end{center}
\end{titlepage}


\subsubsection*{Question 1}

\begin{minipage}{\textwidth}
	\includegraphics[width=\linewidth]{q1_fig1}
	\captionof{figure}{Scatter plot of $x_{n+1}$ against $x_{n}$}
	\label{q1_fig1}
\end{minipage}
\vspace{0.2cm}

A linear relationship between $x_{n}$ seems to be feasible since the data is mostly focussed around a line of best fit of approximately $x_{n+1} = x_{n}$. However, there would be many anomalous points far from this line whose existence could not be explained by $H_{1}$. 

\subsubsection*{Question 2}

Use the model $\bm{Y} = \bm{X\beta} + \bm{\epsilon}$  where, taking $N = 999$,
\begin{align*}
	\bm{Y} = \begin{pmatrix}
		x_{1} \\
		x_{2} \\
		\vdots \\
		x_{N}
	\end{pmatrix}, ~ 
	\bm{X} = \begin{pmatrix}
		1 & x_{0} \\
		1 & x_{1} \\
		\vdots & \vdots \\
		1 & x_{N-1}
	\end{pmatrix}, ~ 
	\bm{\beta} = \begin{pmatrix}
		a_{0} \\
		a_{1}
	\end{pmatrix}
\end{align*}

Then,
\begin{align}
	\bar{\sigma}^{2} = \frac{1}{N} \sum_{n = 0}^{N-1}&(x_{n+1}-\bar{a}_{0}-\bar{a}_{1}x_{n})^{2} \text{ ~~~~~~~~~~~ This  is easy to derive.} \\
	\bm{\bar{\beta}} = (\bm{X}^{T}\bm{X}&)^{-1}\bm{X}^{T}\bm{Y} \text{ ~~~ This is a standard result from lectures.} \nonumber\\
	\text{det} & (\bm{X}^{T}\bm{X}) = |\bm{X}^{T}\bm{X}| = N\sum\limits_{n=0}^{N-1}x_{n}^{2}- \left(\sum\limits_{n=0}^{N-1}x_{n}\right)^{2} \nonumber\\
	(\bm{X}^{T}\bm{X})^{-1} & = |\bm{X}^{T}\bm{X}|^{-1}
	\begin{bmatrix} 
		\sum\limits_{n=0}^{N-1}x_{n}^{2} & -\sum_{n=0}^{N-1}x_{n} \\
		-\sum\limits_{n=0}^{N-1}x_{n} & N
	\end{bmatrix} \nonumber\\
	\implies ~~~
	\bar{a}_{0} & = \frac{1}{ |\bm{X}^{T}\bm{X}| } \left( \sum\limits_{n=0}^{N-1}x_{n+1}\sum\limits_{n=0}^{N-1}x_{n}^{2} - \sum\limits_{n=0}^{N-1}x_{n}\sum\limits_{n=0}^{N-1}x_{n}x_{n+1} \right) \\
	\bar{a}_{1} & = \frac{1}{ |\bm{X}^{T}\bm{X}| } \left( N\sum\limits_{n=0}^{N-1}x_{n}x_{n+1} - \sum\limits_{n=0}^{N-1}x_{n}\sum\limits_{n=0}^{N-1}x_{n+1} \right)
\end{align}

\begin{align*}
	\bar{\sigma}^{2} & \sim \frac{\sigma^{2}}{N}\chi_{N-2}^{2} \text{ ~~~~ From lectures} \\
	\bm{\bar{\beta}} & \sim N_{2}(\bm{\beta}, \sigma^{2}(\bm{X}^{T}\bm{X})^{-1}) \text{ ~~~~ From lectures} \\
	\implies \bar{a}_{0} & \sim N\left( a_{0},  \sigma^{2}|\bm{X}^{T}\bm{X}|^{-1} \sum\limits_{n=0}^{N-1}x_{n} \right) \\
	\bar{a}_{1} & \sim N\left( a_{1},  N\sigma^{2}|\bm{X}^{T}\bm{X}|^{-1} \right)
\end{align*}

Using equations (1), (2) and (3), we calculate the following values (to 3s.f.),
\begin{align*}
	\bar{a}_{0} = -0.0193 \\
	\bar{a}_{1} = 0.893 ~ \\
	\bar{\sigma}^{2} = 1.08 ~~ \\
\end{align*}


\subsubsection*{Question 3}

\begin{itemize}[topsep = 8pt, leftmargin = *]
	\itemsep 0em
	\item For $a_{1} < 0$ (and $a_{0}$ not too large in the case $a_{1}>-1$) $x_{n}$ will oscillate between positive and negative values. This is illustrated in Figure \ref{q3_fig1}. \vspace{0.3cm}\\
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{q3_fig1}
		\captionof{figure}{Behaviour of $x_{n}$ with $a_{0}=1$, $a_{1}=-0.8$, $\sigma^{2} = 1$}
		\label{q3_fig1}
	\end{minipage}
	\item For $|a_{1}|>1$, $x_{n}$ diverges almost surely as $n\to\infty$. We know this is true because it is true of the random walk when $a_{1}=1$ and similarly for $a_{1}=-1$. Then $|a_{1}|>1$ will only act to increase the growth of $x_{n}$ so the divergent behaviour will only be augmented.
	\item For $|a_{1}|<1$, $x_{n}$ will almost surely not diverge and so the data will be focussed in a finite region as $n\to\infty$. This is the case because for $|x_{n}|$ sufficiently large it is almost certain that $|x_{n+1}|<|x_{n}|$. Despite this, the larger the magnitude of $a_{1}$, the more opportunity there is for $x_{n}$ to grow and so there will be a wider variance in the values of the sequence $(x_{n})$.
\end{itemize}


\subsubsection*{Question 4}

\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width = \linewidth]{q4_fig1}
	\captionof{figure}{Behaviour of $x_{n}$ in autocrrelation.dat}
	\label{q4_fig1}
\end{minipage}
\vspace{0.1cm}\\
From Figure \ref{q4_fig1}, we can see that $x_{n}$ is not diverging so we predict $|a_{1}|<1$. Then, using Figure \ref{q1_fig1}, we can predict that $a_{1}>0.5$ since $a_{1}$ represents the gradient of the line of best fit and $0.5$ is the gradient of the line between the points $(-4, -2)$ and $(4, 2)$. % In addition, the data is not oscillating between positive and negative values in the way it would if $a_{1}<0$ % but you said this depends on a_0 as well
So take $R = (0.5,1)$. From Question 2,
% Look at question 1
\begin{align*}
 	\bar{a}_{1} \sim N\left( a_{1}, \sigma^{2}|\bm{X}^{T}\bm{X}|^{-1} N \right) & \\ % sigma squared NOT INVERSE
 	\text{Let, SE}(\bar{a}_{1}) = \sqrt{\tilde{\sigma}^{2}|\bm{X}^{T}\bm{X}|^{-1} N} \text{ where } \tilde{\sigma}^{2} & = \text{ RSS }/(N-2) = N\bar{\sigma}^{2}/(N-2). \\
	\text{Then, } \frac{ \bar{a}_{1} - a_{1} }{ \text{SE}(\bar{a}_{1}) } = \frac{ \bar{a}_{1} - a_{1} }{ \sqrt{\tilde{\sigma}^{2} N|\bm{X}^{T}\bm{X}|^{-1}} } & = \frac{ (\bar{a}_{1} - a_{1}) / \sqrt{\sigma^{2} N|\bm{X}^{T}\bm{X}|^{-1}} }{ \sqrt{ N\bar{\sigma}^{2} / ((N-2)\sigma^{2}) } } \\
	\text{Observe that the numerator is a standard normal} & \text{ N(0,1) and the denominator is an independent} \\
	\sqrt{ \chi^{2}_{N-2} / (N-2) }. \text{ ~ Thus, } & \\
	\frac{ \bar{a}_{1} - a_{1} }{ \text{SE}(\bar{a}_{1}) } \sim t_{N-2} & \\
	\implies a_{1} \sim \bar{a}_{1} - \text{SE}&(\bar{a}_{1})t_{N-2} \\
	\text{SE}(\bar{a}_{1}) = 0.01427, \bar{a}_{1} & = 0.893 \\
	\text{Then, } t_{N-2} = 27.5, ~ t_{N-2} = -7.47 \text{ for } \bar{a}_{1} - & \text{SE}(\bar{a}_{1})t_{N-2} = 0.5 \text{ and } 1 \text{ respectively.} \\
	P(-7.47 \leq t_{997} \leq 27.5) & = 1
\end{align*}
And so this is the probability $a_{1}$ lies in R. \\

% The method above does NOT use the fact that the x_n are a chain so maybe that's why its off ?? - seems so since otherwise is correct. Maybe the Markov chain density lemma 1 will help.  - THIS IS WRONG - we assume H1 to be true so in fact don't need to use the chain idea, just need to find best estimate for a_1 to make the list x_{n+1} match the list from x_{n} 
% Did use t dist as well as a_1_bar and sigma_bar as the question says to so seems right
% Then the fact the probability is so low means its just as likely |a_1| => 1, which we know is extremely unlikely to generate the x_n. So |a_1| < 1 is now equally ridiculous so can be rejected.


\subsubsection*{Question 5}

\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width = \linewidth]{q5_fig1}
	\captionof{figure}{Histogram of $x_{n}$}
	\label{q5_fig1}
\end{minipage}
\\

Using the sum of normal distributions and the fact that if $X \sim N(\mu, \sigma^{2})$ then $aX \sim N(a\mu, (a\sigma)^{2})$ we obtain,
\begin{align*}
	X_{n} \sim & N\left( a_{0}\left(1+a_{1}+...+a_{1}^{n-1}\right), \sigma^{2}\left( 1+a_{1}^{2}+...+a_{1}^{2(n-1)} \right) \right) \text{ ~~~ For $n\geq1$} \\
	& \implies ~ X_{n} \sim N\left( a_{0}\frac{1-a_{1}^{n}}{1-a_{1}}, \sigma^{2}\frac{1-a_{1}^{2n}}{1-a_{1}^{2}} \right) \\
	& \implies X_{n} \sim N\left( \frac{a_{0}}{1-a_{1}}, \frac{\sigma^{2}}{1-a_{1}^{2}} \right) \text{ ~~~~~~ For $n$ large and $|a_{1}|<1$}
\end{align*}

Thus for our data, since there are $1000$ data points, $n$ is large for most $x_{n}$ so we should expect the histogram to resemble a normal distribution. However, Figure \ref{q5_fig1} has two separate large peaks so is very unlikely to have been caused by a normal distribution of data. Thus the hypothesis $H_{1}$ no longer seems reasonable.
% and/or say data not peaking around the mean
% Second explanation: x=x a_1x+a_0 only has 1 equilibrium point but histogram suggests there are two such points


\subsubsection*{Question 6}

Using the model $\bm{Y} = \bm{X\beta} + \bm{\epsilon}$, we have,
\begin{align*}
	\tilde{\sigma}^{2} & = \frac{1}{N} \sum_{n = 0}^{N-1}(x_{n+1} - \tilde{a}_{0} - \tilde{a}_{1}x_{n} - \tilde{a}_{2}x_{n}^{2})^{2} \\
	\bm{\tilde{\beta}} & = (\bm{X}^{T}\bm{X})^{-1}\bm{X}^{T}\bm{Y} \\
\end{align*} 
Then the values to 3s.f. are,
\begin{align*}
	\tilde{a}_{0} & = 0.146 ~~~~ \\
	\tilde{a}_{1} & = 0.865 ~~~~ \\
	\tilde{a}_{2} & = -0.0318 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\
	\tilde{\sigma}^{2} & = 1.062
\end{align*}

\subsubsection*{Question 7}

\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width = \linewidth]{q7_fig1}
	\captionof{figure}{Plots of $y=f(x)$ and $y = g(x)$}
	\label{q7_fig1}
\end{minipage}
\vspace{0.2cm}\\
Solving $\bar{a}_{2}x^{2}+\bar{a}_{1}x+a_{0} = x$ with the quadratic formula, we obtain the roots,
\begin{align*}
	x = -5.14, ~~~ x = 0.890
\end{align*} 
% -5.13897, 0.890525 exactly

$x=-5.14$ comes before the first large peak on the histogram in Figure \ref{q5_fig1} while $x=0.890$ is at the start of the second peak. $x=0.890$ is an attractor of the system $x_{n+1}=f(x_{n})$. % See Q8 (put in write up?)
Then, as the data is drawn to $0.890$, the $N(0,\sigma^{2})$ term will mean you should expect to see the data split in half above and below this fixed point which is exactly the case in the histogram.
%  In addition, there are very few points $<-5.17$ because 0.856 is attractor
% But the points below would -> -inf

\subsubsection*{Question 8}

\begin{align*}
	f'(x) & = 0.865 - 0.0637x \\
	\text{In particular, } f'(-5.14) & = 1.19, ~~ f'(0.890) = 0.808
\end{align*}
A fixed point $t$ of $x_{n+1}=f(x_{n})$ is stable if $|f'(t)|<1$ and unstable if $|f'(t)|>1$. Therefore, the chain $x_n$ is expected to approach 0.865 and diverge away from -5.17. Then, with this model we would most likely see a single peak at $x=0.865$. This is further supported by the fact that the curve $y=f(x)$ in the range $-6<x<5$ can be approximated by a straight line. We find that the line $y=h(x)=0.897x-0.168$ (found by minimising the MSE between itself and a sample generated with $f(x)$) is a good approximation as shown in Figure \ref{q8_fig1}. \\
\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width = 0.65\linewidth]{q8_fig1}
	\captionof{figure}{Plots of $y=f(x)$ and $y = h(x)$ in range $(-6, 5)$}
	\label{q8_fig1}
\end{minipage}
\vspace{0.2cm}

Thus $H_{2}$ approximates to a linear model. Then as with the hypothesis $H_{1}$, you would expect a normally distributed histogram with the peak at the fixed point $x=0.865$ but this is not the case. Thus $H_{2}$ is also implausible.
% There's also the points < -5.17 to consider but it is plausible that they happen to bounce back up before diverging to -inf so we will ignore this

\subsubsection*{Question 9}

Again using the model $\bm{Y} = \bm{X\beta} + \bm{\epsilon}$, we have,
\begin{align*}
	\hat{a}_{0} & = 0.315  \\
	\hat{a}_{1} & = 1.25 \\
	\hat{a}_{2} & = -0.101 \\
	\hat{a}_{3} & = -0.0474 \\
	\hat{\sigma}^{2} & = \frac{1}{N} \sum_{n = 0}^{N-1}(x_{n+1} - \bar{a}_{0} - \bar{a}_{1}x_{n} - \bar{a}_{2}x_{n}^{2} - \hat{a}_{3}x_{n}^{3} )^{2} \\
	& = 0.560 \\
	\\
	\bar{a}_{i} & \sim N\left( a_{1},  \sigma^{2}(\bm{X}^{T}\bm{X})^{-1}_{ii} \right) \\
	\text{Then ~} \sigma^{2}_{0}(\bm{X}^{T}\bm{X})^{-1}_{ii} & \text{~ is an unbiased variance estimator if } \mathbb{E}(\sigma^{2}_{0}) = \sigma^{2}. \\
	\text{~ This is} & \text{ true for ~} \sigma^{2}_{0} = \text{RSS}/(N-4) = 0.562 \\
	(\bm{X}^{T}\bm{X})^{-1}_{11} & = 2.51\times10^{-3}, ~ (\bm{X}^{T}\bm{X})^{-1}_{22} = 5.31\times10^{-4}, \\ (\bm{X}^{T}\bm{X})^{-1}_{33} & = 6.34\times10^{-5}, ~ (\bm{X}^{T}\bm{X})^{-1}_{44} = 4.48\times10^{-6} \\
	\text{Thus w} & \text{e have the following unbiased estimators, ~} \\
	\text{var}(\hat{a}_{1}) = 1.41\times10^{-3}, & ~ \text{var}(\hat{a}_{2}) = 2.99\times10^{-4}, ~ \text{var}(\hat{a}_{3}) = 3.56\times10^{-5}, ~ \text{var}(\hat{a}_{4}) = 2.52\times10^{-6}
\end{align*}


\subsubsection*{Question 10}

The distribution function of an exponential random variable with mean 2 is $F(x) = 1-e^{-x/2}$. Using the inverse of this function, we can generate a sample of 1000 exponential random variables. An example output to the program is shown below:
\vspace{0.2cm}\\
\begin{minipage}{\textwidth}
	\centering
	\includegraphics[width = 0.5\linewidth]{q10_fig1}
	\label{q10_fig1}
\end{minipage}


\subsubsection*{Question 11}

\begin{enumerate}[label=(\roman*)]
	\item $~~~~~~~~$
	\begin{minipage}{\textwidth}
		\includegraphics[width = 0.9\linewidth]{q11_fig1}
		\label{q11_fig1}
	\end{minipage}
	\item $~~~~~~~~$
	\begin{minipage}{\textwidth}
		\includegraphics[width = 0.6\linewidth]{q11_fig2}
		\label{q11_fig2}
	\end{minipage}
	\item $~~~~~~~~$
	\begin{minipage}{\textwidth}
		\includegraphics[width = 0.6\linewidth]{q11_fig3}
		\label{q11_fig3}
	\end{minipage}
\end{enumerate}

The histogram in Figure \ref{q5_fig1} shows resemblance to the histogram in (ii) as they both have peaks at -3 and 2. Likewise, the plot in Figure \ref{q1_fig1} is similar to diagram (iii) as both have a concentration of points at (-3, -3) and (2,2). Due to the random nature of the model, we do not expect the plots in (i) to match. Nevertheless, they are similar in that they have sharp up and downward spikes. This all provides evidence to support the model as any good model should have these similarities.
% you would expect the properties of (i), (ii) and (iii) to be preserved by any good model.
\\

On the other hand, for (i) the value of $S_{1000}$ is about 250 for the model data and about -200 for the actual data which is a large discrepancy. Running the model multiple times, I find that we almost always have a positive value of $S_{1000}$ suggesting that there are too many large $y_{i}$; this is supported by the fact that the second peak in (ii) is much larger than the first which is not the case in Figure \ref{q5_fig1}. In addition, the points in (iii) are more spread out than in Figure \ref{q1_fig1}. Therefore, overall I find the model to be unsatisfactory as one would expect these properties to be more likely to be preserved.

% In addition, the fact that $y_{n}$ becomes too large very quickly when escaping from attractors means ??? Overall, I find the model to be unsatisfactory.


\pagebreak
\textbf{question\textunderscore1.py for question 1}\centering\label{question1}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_1.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore2.py for question 2}\centering\label{question2}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_2.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore3.py for question 3}\centering\label{question3}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_3.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore4.py for question 4}\centering\label{question4}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_4.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore5.py for question 5}\centering\label{question5}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_5.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore6.py for question 6}\centering\label{question6}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_6.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore7.py for question 7}\centering\label{question7}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_7.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore8.py for question 8}\centering\label{question8}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_8.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore9.py for question 9}\centering\label{question9}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_9.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore10.py for question 10}\centering\label{question10}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_10.py}
\vspace{2cm}

\pagebreak
\textbf{question\textunderscore11.py for question 11}\centering\label{question11}
\lstinputlisting[language=Python]{C:/Users/angus/OneDrive/Documents/CATAM 2.4 Code/question_11.py}
\vspace{2cm}

\end{document}

